SAFE Requirements

*Notes*:
- the methodology work is optional material for a PhD or proposal

%contents


%1 Introduction

A modern factory engaged in the production of high-tech goods combines a
large degree of automation with skilled labour of various types and
quantities. Robots often play a significant role, although they are never
altogether unaccompanied: at the very least service engineers must attend
their operation, and in most cases there will also be staff who take care of
reconfiguring robotic equipment for new product lines or refinements to the
existing processes and products.

SAFE, the Semantic Annotation Factory Environment, is a software
suite and a methodology for the implementation and support of *Annotation
Factories*. SAFE is a novel development in several respects, because
o it unifies previously disjoint IE adaptation interventions
o it complements GATE's developer-oriented facilties with a UI oriented on
  other user groups
o it is methodological instead of purely technological 

The rest of this section addresses these points in turn, and summarises the
structure of this document.


%2 Unifying Adaptation Interventions for Custom Extraction Services (CUES)

Information Extraction (IE) is the process of automatically analysing text
(and, less often, speech) to populate fixed-format, unambiguous data
structures such as spreadsheets, link-analysis engines or databases. An IE
system is a dynamic object which represents a compromise between information
need and development cost. Almost never do the information need and the data
remain static. Therefore applications software using IE has to provide a
mechanism to tailor the extraction components to new information needs and to
new types of data. Up to now, no single mechanism has been discovered that
covers all cases, meaning that IE software has to support a wide spectrum of
adaptation interventions, from skilled computational linguists, through
content administrators and technical authors, to unskilled end-users
providing behavioural exemplars. In each area significant research streams
exist and have made good progress over the past decade or so. What has not
been done is to construct a unified environment in which all the different
adaptation interventions work together in a complementary manner.

The three principal mechanisms for IE adaptation in leading HLT research
streams are:
- assisted authoring of finite state transduction rules, including: generation
  of rules from annotation pattern searches across corpora (annotations in
  context); graphical debugging of transduction rule execution
- supervised mixed-initiative learning of extraction models
- unsupervised clustering of terms for purposes such as populating
  domain-specfic gazetteers

In addition, underlying the adaptation process are automated measurement and
visualisation tools, and frequency-based document search (or information
retrieval) tools. 


%2 Beyond Language Engineering

GATE has both a class library for programmers embedding LE in applications
and a development environment for skilled language engineers. SAFE, however,
has to support a wider constituency of users. There are two main cases.

First, annotation of training data for learning algorithms should be a task
requiring little skill beyond that of a computer-literate person. That's a
requirement because training data volumes are typically large and therefore
the labour involved has to be cheap to make the process economic. For the
same reason the annotation environment should be as productive as possible,
for example by bootstrapping the annotation process with mixed-initiative
learning and by providing a voting mechanism for multiple simultaneous
annotators (this is necessary to guarantee quality with low-skilled staff).

Second, data curation or systems administration staff may become involved in
customising extraction systems. These types of people would take a training
course of a week or two, and be expected to use a richer toolset, perhaps
including things like ANNIC-based JAPE rule authoring.


%2 IE: the Missing Manual

Extraction is not an application in itself, but a component of
information seeking and management tasks. Despite the breadth and depth of
literature describing algorithms, evaluation protocols and performance
statistics for IE the technology lacks a clear statement of how to go about
specifying and implementing IE functionality for a new task or domain. In the
same way that GATE is not just an implementation but also an abstract
architecture, so SAFE can increase its impact by defining a methodology.

The methodology will cover:
- how to decide if IE is applicable to your problem
- how to define the problem with reference to a corpus of examples
- how to identify similarities with other problems and thence to estimate
  likely performance levels
- how to design a balance of adaptation interventions for system setup and
  maintenance


%2 Structure of this Document

The rest of this document
- discusses applications that may be used to trial SAFE and guide its 
  development [moved to %(safe.html)]
- lists requirements derived from the applications
- notes various implementation choices


%1 Requirements

%2 User profiles

The applications described above suggest different types of user profiles:
- Annotators
- Language Engineers
- Information Curators 

Depending on the context, a user could have more than one profile, being for
instance both Language Engineer and Information Curator.

%3 Annotators

Annotators are in charge of annotating entities, relations or events on a set
of documents with regard to an ontology or to a flat list of named entities.
They must be able to access the documents remotely on the web. The annotated
documents can be stored directly or used to bootstrap a Machine Learning
system. The Annotator interface would probably include some information about
the number of remaining documents to be annotated and some basic message
system for interacting with an Information Curator.

Different Annotators may work on a single corpus in order to speed up the
process of annotation. They may possibly annotate the same documents, in
order to make sure that the quality of the annotation is optimal, and to
evaluate Annotator performance.

Some tools would be added in order to speed-up the annotation of documents,
for example, the possibility to define positive or negative gazeteers. A
positive gazeteer would for instance specify that for a given ontology, all
instances of _Calvin Klein_ in the corpus need to be annotated as being a
_Brand_. A negative gazeteer would specify that _Calvin Klein_ is never to be
annotated as a _Person_.

Another functionality would be to display some or all the occurences of a
given string in the corpus. This would be useful in order to build the
gazeteers mentionned above, the annotator could thus make sure that a given
pattern is regular (e.g. _Calvin Klein_ is always a _Brand_).

A Mixed Initiative system can be set up by an Information Curator or a
Language Engineer and used by an Annotator. This means that once a document
has been annotated manually, it will be used to generate a Machine Learning
model. This model will then be applied on any new document, so that this
document will be partially or totally annotated. The Annotator would just
have to correct or validate the annotations provided by the ML system. This
should make the annotation task much faster. The negative gazeteers defined
by the Annotator would be applied after the Mixed Initiative annotation.

*Note : what about leaving the creation of gazeteers to the Language Engineer or 
Information Curators?
Annotators deal with documents taken one by one and may not necessarily need to 
access a corpus as a whole. But it would be possible to record the things
added / deleted by the Annotators and create gazeteers with that. Plus, getting
this information from multiple annotators would make it more relevant than
having a gazeteer per annotator.
*

%3 Language Engineers

Language Engineers have some knowledge of Linguistic Engineering, thus they
are able to create or modify a set of linguistic resources. Unlike the
annotators, the developers would work on resources producing automatic
annotations. These resources are combined to produce a service. Ultimately
this service will be accessible on-line to the end user. Like the Annotators,
Language Engineers can be located in very different parts of the world, and
thus they must access the system remotely.

*Functionalities*:
- ANNIC and ANNIC-based JAPE rule authoring
- JAPE rule editing and debugging
- Gazeteers
- Machine Learning (including the use of models generated by Mixed
  Initiative: see above)
- Evaluation of performance for a whole service or by processing resource


%3 Information Curators

Information Curators are in charge on the configuration of the system. Being
based on a Service Oriented Architecture, new services can be added to the
network.  Such services can be for instance a storage of corpora or
processing ressources, or an annotation service taking as input one/more
documents and applying one/more processing ressources. The Information
Curator can get information about the performances of a service, by keeping a
part of the data for evaluation, or about performance of the manual
annotation. The latter means that the Information Curator would be informed
in case one of the Annotator provides annotations which systematically differ
from the other Annotators. This way a better explanation of its task could be
sent to the Annotator, which should improve the overall quality of the
annotation.

*Functionalities*:
- Building services by combining available resources, including manual
annotation
- Expose the output of a service to the customers: e.g. create a WSDL
- Managing data: create corpora and documents
- Evaluating the performance of a service
- Monitor the performance of the Annotators + supervise them


%2 Architecture

SAFE will be implemented as a Service Oriented Architecture. A
service-oriented architecture is an information technology approach or
strategy in which applications make use of (perhaps more accurately, rely on)
services available in a network such as the Internet. Implementing a
service-oriented architecture can involve developing applications that use
services, making applications available as services so that other
applications can use those services, or both.

The concept of an SOA is not new. Service-oriented architectures have been
used for years. What distinguishes an SOA from other architectures is loose
coupling. Loose coupling means that the client of a service is essentially
independent of the service. The way a client (which can be another service)
communicates with the service doesn't depend on the implementation of the
service. Significantly, this means that the client doesn't have to know very
much about the service to use it. For instance, the client doesn't need to
know what language the service is coded in or what platform the service runs
on. The client communicates with the service according to a specified,
well-defined interface, and then leaves it up to the service implementation
to perform the necessary processing.

Gartner reports that _"By 2008, SOA will be a prevailing software engineering
practice, ending the 40-year domination of monolithic software architecture"_
and that _"Through 2008, SOA and web services will be implemented together in
more than 75 percent of new SOA or web services projects."_

One of the advantages of such an architecture is that it makes the building
of a distributed application easier. This is particularly crucial for
CPU-intensive tasks such as Machine Learning and can help with scalability.

The picture below illustrates the SOA for SAFE. The letter A next to the box
representing the Annotator GUI indicates that the GUI is an browser based
interface, B indicates that it is a standalone application. The latter connects
directly to the SAFE service, the former communicates with it through a web
application. The description of these two approaches is given below.

%image(images/gleam-soa.png)


%2 Functionalities

This chapter lists the functionalities required in SAFE, how they can be
decomposed into atomic actions and how they are related to existing resources.

%3 Manual annotation

*NOTE* Difference between OLLIE and SAFE is probably the frequency of 
information returned to the server.

From *Language engineering tools for collaborative corpus annotation*:

_OLLIE supports collaborative annotation of documents and corpora by allowing 
their shared, remote use and by making updates made by one client immediately
available on the OLLIE server. In this way users can share the annotation task 
with other users. For example, one user can annotate a text with organisations,
and then another annotates it with locations. The documents reside on the 
shared server which means that one user can see errors or questionable mark-up
introduced by another user and initiate a discussion._ 

There is one event fired per annotation added and an instant synchronization
with the server. SAFE will behave probably like OBIE where the whole document
is sent to a MixedInitiative Engine. This would reduce the amount of data
exchanged between the client and the service.

%3 Connection to a MIEngine

This functionality is supported in OLLIE. In SAFE, the Annotator 
GUI will
request a document to be annotated from the SAFE service. If a Mixed Initiative
engine has been set and performs better than a fixed threshold, the document
will be pre-processed with some PR and annotated with the Mixed Initiative
Engine. Thus the Annotator will get partially or fully annotated documents. The
pre / post-processing includes the use of user-defined gazetteers (see below). 
The processing made before the Annotator gets the document is not set up by him,
but by an Information Curator. From this point of view, the presence of a Mixed
Initiative Engine, or any other form of processing, is known to the Annotator
only by the fact that the documents may appear on its GUI with some annotations.

%3 ANNIC

ANNIC is a functionality of GATE which given a JAPE grammar expression, returns
all the occurences of the pattern in a corpus. This functionality is helpful for
creating JAPE rules or gazeteers. (The difference between the two is that the
latter probably requires only surface forms while the former may need other
annotation types.) We probably need to define a Search API: the search
functionality is currently outside the storage. (The Ontotext guys combined the
two things with their Lucene-based DataStorage.)

%3 Assisted creation of positive/negative gazeteers

Positive/negative gazeteers are created by Annotators (or Information Curators?)
and are applied before/after automatic annotations e.g. Machine Learning. 
Positive gazeteers mark spans of text with regard to a given ontology, negative 
ones prevent a given span of text to have a specific annotation.

*Note* where to store these lists? One by annotator / ontology? or just a shared
one / ontology?


%3 Identication of user

All users have to be identified in order e.g. to keep a different version of the
annotated documents for each Annotator, or to manage access rights to the
resources.


%3 Reading messages + guidelines?

Rely on external tools: mail + messengers + ....


%3 ANNIC-based JAPE rule authoring?


%3 Classic PR's from GATE.

JAPE rule editing and debugging ?
Gazeteers
*Note* the question is: do we really need to have a Web-based interface for
Language Engineers? Can't they just use GATE and expose the application they
build as Web Services? We can extend SAFE for them later...


%3 Machine Learning

(including the use of models generated by Mixed Initiative)


%3 Managing data: create corpora and documents

On WS-based DataStorage


%3 Building services

by combining available resources (i-e ), including manual annotation
and expose the output of a service to the customers: e.g. create a WSDL


%3 Evaluating the performance of a whole service

by defining a part of the resources to be used as a reference corpus (i-e 
annotated by Annotators).


%3 Register new users

An interface will be created in order to manage information about the users,
their profiles, and so on.


%3 Assign a task to an Annotator

An Information Curator can assign a list of documents to be annotated by an
Annotator, or define a mechanism that automatically spreads the work load on the
different annotators. The same document may be annotated several times by
different people, in which case an inter-annotator measure would tell how
coherent the annotations are (see below).

As mentionned by Borislav, one way to look at human Annotators is to consider
them as extensions of an automatic system, i-e the system would ask an Annotator
to correct/add information when the quality of the annotation is detected as
being not good enough. The manual annotation is the *triggered* by the system. A
mechanism is then needed to detect such cases.

*TODO formalise the different types of interactions between the system / the 
annotator*


%3 Monitor the performance of the Annotators

In case the Annotators annotate the same documents: make sure that the
annotation is correct by comparing with the documents produced by other 
Annotators. The comparison has to be done before using the Annotations for ML.
There may be a problem in applying that before Mixed-Initiative.
There will be several mechanisms: one for generating the best possible document 
from the different versions (using some kind of voting mechanism) *or* resend a
document to the user until the quality is good enough (*vicious but
efficient?*).


