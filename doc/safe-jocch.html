<?xml version="1.0" encoding="utf-8"?>
<!-- AUTOGENERATED FILE: ALL EDITS WILL BE LOST!!! -->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Semantic Annotation Factories for Cultural Heritage</title>
<style type="text/css">
/*<![CDATA[*/
<!--
html, body {
  background: #fff;
  color: #000;
  font-family: sans-serif;
}
h1,h2,h3,h4,h5,p,ul,ol { font-family: sans-serif; }
pre { font-family: monospace; }
h3.navhead {
  font-size: 100%;
}
div.banner {
  border: none;
  margin-right: 0px;
  margin-left: 0px;
  padding: 0.09em;
  text-align: center;
  font-weight: bold; 
}
div.banner a:link, div.banner {
  background: #A0D0F0;
  color: #000000;
}
div.banner a:active {
  background: #000000;
  color: #FFFFFF;
}
div.banner a:hover {
  background: #000000;
  color: #FFFFFF;
-->
/*]]>*/
</style>
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Semantic Annotation Factories for Cultural Heritage</h1>

<p>Hamish Cunningham, Kalina Bontcheva, Valentin Tablan <br/>
Department of Computer Science, <br/>
University of Sheffield</p>

<p><h2>Contents</h2>
<p><ul>
<li><a href="#section-1.">1. Introduction</a></li>
<li><a href="#section-2.">2. Semantic Annotation Successes</a></li>
<li><a href="#section-3.">3. Trouble in Paradise: Why Isn't Semantic Annotation More Widespread?</a></li>
<li><a href="#section-4.">4. From Annotation Tools to <em>Annotation Factories</em></a></li>
<li>&nbsp;&nbsp;<a href="#section-4.1.">4.1. Annotation: the Missing Manual</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.2.">4.2. A Division of Labour (Multi-role Methodology)</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.3.">4.3. Unifying Adaptation Interventions</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.4.">4.4. Assistive vs. Autonomous</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.5.">4.5. Iterative Processes vs. Waterfall or "Big Bang"</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.6.">4.6. Service-Orientated vs. Monolithic</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.7.">4.7. Sustainability, Open Source, Community Support</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.8.">4.8. General vs. specific</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.9.">4.9. Grounded</a></li>
<li><a href="#section-5.">5. Conclusion</a></li>
<li><a href="#section-6.">6. Postscript</a></li>
<li><a href="#section-7.">7. Misc text that may not fit anywhere</a></li>
<li>&nbsp;&nbsp;<a href="#section-7.1.">7.1. Semantic Annotation and Human Language Processing</a></li>
<li>&nbsp;&nbsp;<a href="#section-7.2.">7.2. Implementation</a></li>
<li>&nbsp;&nbsp;<a href="#section-7.3.">7.3. Semantic Web Standards and Semantic Annotation</a></li>
</ul></p></p>

<a name="section-1."/><h1>1. Introduction</h1>

<p>Consider three characteristics of cultural artefacts, their study and their
manipulation in a digital world:</p>

<p>First, many routine actions that scholars perform can be modelled
computationally as annotation. For example:</p>
<ul>
<li>Reference to other works; note taking; summarising; citation; quotation;
  footnotes.</li>
<li>The apparatus of critical editions describing variants, sources,
  cross-references.</li>
<li>Transcription of manuscript images; formal description and cataloging;
  documentation of artifact/folio additions, changes, deletions.</li>
</ul>
<p>When modelled as annotation machines can help us act on the documents in new
ways as <b>living editions</b> that capture multiple viewpoints and flexibly
combine multiple levels of interpretation and description.</p>

<p>Second, popular websites such as Flikr and Delicious also use annotation in
the form of collaborative tagging (or <em>folksonomies</em>). Wikipedia and other
wikis and blogs use annotation for article interlinking, backlinks and so on.
These technologies are successful because they promote collaboration and feed
back the results of the annotation process to improve their services (for
example users of Delicious get more efficient at managing and retrieving
bookmarked web pages).</p>

<p>Third, formal and authoritative annotation by scholars is most useful when
accompanied by a machine-interpretable semantics or ontology. Just as XML has
transformed the interchange of structured data by providing a universal
syntax, the richer schema languages developed in semantic web research can
facilitate the provision of higher-level information services. (They are not
a panacea, of course, but have certainly demonstrated their worth in a number
of scenarios.)</p>

<p>This paper, then, is about <b>collaborative semantic annotation</b>.</p>

<p>The University of Sheffield and others have developed and deployed semantic
annotation systems for diverse purposes using
<a href="http://gate.ac.uk/">GATE</a>, a General Architecture for Text Engineering.
This technology has been used to supply a range of web services and tools to a
wide community of users and developers. Here we'll look at how these types of
services might be adapted and extended for a Cultural Heritage (CH) agenda and
propose a Semantic Annotation Factory Environment (SAFE) for the creation and
evolution of metadata supporting rich search, navigation, browsing and
collaborative critical commentary over CH collections.</p>

<p>The rest of the paper</p>
<ul>
<li>looks at some recent successes of semantic annotation (section 2)</li>
<li>discusses why the technology is not spreading as rapidly as might be
  expected (section 3)</li>
<li>describes the ways in which systems need to change in order to promote
  easier adoption (section 4)</li>
</ul>


<a name="section-2."/><h1>2. Semantic Annotation Successes</h1>

<p>Semantic Annotation is about attaching meaningful structures to resources like
documents or video streams in such a way that they can be used by computers to
enhance the usefulness of those resources.</p>

<p>One simple kind of annotation that has been used in the Perseus digital
library is named entity annotation. This is where the dates, names of people
and places and so on that appear in the documents (in this case one of the
largest collections of classical literature) are annotated. This allows
Perseus to do some of the things illustrated in these screen shots: at the top
there is a map of references to places in the literature, which can also be
graphed over time; below there is a new search tool that allows you to specify
particular people, or locations, etc.</p>

<table border="1">
  <tr><td> <img src="http://gate.ac.uk/sale/images/perseus-maps.PNG"/>              </td></tr>
  <tr><td> <img src="http://gate.ac.uk/sale/images/perseus-name-search.PNG"/>       </td></tr>
</table>

<p>A second semantic annotation success story arose as an element of ``Web 2.0'',
viz. the annotation of web resources via a process of collaborative tagging.
The resultant community-maintained metadata has come to be known as a
<em>folksonomy</em> and has been notably successful in bookmark and image sharing.
<a href="http://del.icio.us/">Delicio.us</a> is a shared bookmarks hosting service
which exploits redundancy in the web to generalise and to facilitate
categorisation and search of bookmark lists.</p>

<p><a href="http://www.flickr.com">Flickr</a> is a photo tagging and publishing service
"solves" the image analysis problem by soliciting annotation from users which
then serve as search terms for retrieval purposes.
In both cases the resultant folksonomy constitutes annotation as behaviour
mining (or DIY indexing) and is a significant current application of
annotation.</p>

<p>More complex models of annotation are also proving successful, for example in
companies specialising in extracting specific and detailed information from
the web. When combined with structured data sources this annotation can meet
a variety of emerging needs for enhanced control, security and access.
Examples include</p>
<ul>
<li><a href="http://garlik.com">Garlik</a>, which mines data about consumers present in
  various sources including the web</li>
<li><a href="http://innovantage.co.uk">Innovantage</a>, which mines job advertisement data</li>
</ul>

<p>The service provided by Garlik (a 2006 start-up company from the founders of
Egg PLC, an early Internet bank) is to answer the question "who's saying what
about me?" and therefore to give individuals more power over their personal
information.</p>

<p>Innovantage is another new company who provide services to the UK recruitment
industry based on mining the web for employment vacancies. The process:</p>
<ul>
<li>searches for vacancies from 150,000+ companies across all UK industry
  sectors</li>
<li>annotates information about jobs by SIC code, location, job title, essential
  skills, salary, reference, contact etc.</li>
<li>uses ontology-based information extraction and conceptual search based on
  GATE and KIM from OntoText (KIM - <a href="http://www.ontotext.com/kim">http://www.ontotext.com/kim</a>
  <a href="http://gate.ac.uk/sale/bib/main.html#XKiryakov04a">Kiryakov04a</a>, <a href="http://gate.ac.uk/sale/bib/main.html#XPop04a">Pop04a</a> - was evaluated the leading semantic annotation
  tool at the eponymous ISWC 2005 workshop)</li>
</ul>

<p>The result is a service which increases productivity in recruitment agencies
and complements existing structured data sources used by those agencies.</p>


<a name="section-3."/><h1>3. Trouble in Paradise: Why Isn't Semantic Annotation More Widespread?</h1>

<p>Given that annotation has a number of significant uses, why isn't it in use
more widely?</p>

<p>First, most cases aren't as bounded as Perseus, where only classics texts are
dealt with. For example, the <a href="http://www.prestospace.org">PrestoSpace
project</a> for European audio-visual archive preservation estimates the contents
of these archives at </p>

  <table border="1">
  <tr><td> <b>Carrier type</b>                  </td><td> <b># items</b>   </td></tr>
  <tr><td> Film in Broadcast Archives      </td><td> 4.238.857   </td></tr>
  <tr><td> Film in Film Archives           </td><td> 1.665.708   </td></tr>
  <tr><td> Video tapes                     </td><td> 6.232.352   </td></tr>
  <tr><td> Audio recordings                </td><td> 5.321.301   </td></tr>
  <tr><td> Total audiovisual items         </td><td> 17.458.218  </td></tr>
</table>

<p>If we guesstimate around 20-minute per item for a human being to manually
annotate these materials we would need around <b>6 million person-hours</b> -
clearly an infeasible quantity. If we extrapolate from the time takeing in
archiving high-value video materials like the BBC's Newsnight programme where
the rate is eight hours per hour we get a figure of <b>18 million person years</b>!</p>

<p>Secondly, there aren't enough DIY case.  Flikr and Delicio.us have tapped into
obvious and widespread needs, and Innovantage and Garlik hope to do the same,
but in many cases we don't have the necessary folk for a folksonomy.
Broadcast archives, for example, suffer from this Catch-22: </p>
<ul>
<li>you don't know you need it until you use it,</li>
<li>you can't use it until widespread need generates funding.</li>
</ul>

<p>Thirdly, fully automatic annotation processes suffer from what we may refer to
as Square Fish Syndrome. Imagine that you're looking at a river under which a
fish swims by leaving traces of its passing in ripples and eddies on the
surface of the water. What does the fish look like, you wonder?  Two of the
options for finding out are to show the ripples and eddies</p>
<ul>
<li>to an artist and ask them to draw a fish</li>
<li>to a statistician and ask them to model a fish.</li>
</ul>

<p>In both case you're liable to get a square fish, because both methods only
approximate to the deep structure of the river and its denizens using purely
surface phenomema. Similarly, human language processing by machine can be done
by</p>
<ul>
<li>linguists intuiting about grammar and codifying their theory in parsers of
  one sort or another</li>
<li>machine learning creating statistical models from annotated corpora.</li>
</ul>

<p>Unfortunately language also appears to be a surface phenomenon, and in
practice the accuracy of these machine processing is often not sufficient for
practical applications. This leads to a trade-off between application domain
specificity and task complexity:</p>

<p><img src="http://gate.ac.uk/sale/images/spec-vs-comp.png" alt=""Specificity vs.
Complexity"" width="900"/></p>

<p>In this diagram useful performance levels lie beneath the curve: if you have
a complex task, for example extracting opinions from web forums, you can only
expect useable performance if the opinions that you're interested in a very
specific and the language they are expressed in is in some way predictable.</p>

<p>Simple tasks include document clustering, full-text search, entities, simple
descriptions. Complex tasks include relations and events extraction,
cross-document reference resolution, opinion mining.  Examples of specific
domains might be chemical engineering job descriptions, football match
reports. Examples of general domains might be all news sites, a corporate
intranet, the web.</p>

<p>In sum, these three factors currently form barriers to the uptake of semantic
annotation. The rest of the paper considers technological and methodological
solutions to this problem.</p>


<a name="section-4."/><h1>4. From Annotation Tools to <em>Annotation Factories</em></h1>

<p>This section discusses a number of steps that can make semantic annotation
more applicable to humanities information processing.</p>

<p>A note on terminology: below we will sometimes refer to the technology that
allows automation of semantic annotation which is derived from Information
Extraction (IE <a href="http://gate.ac.uk/sale/bib/main.html#XCun05a">Cun05a</a>, <a href="http://gate.ac.uk/sale/bib/main.html#XGri96">Gri96</a>, <a href="http://gate.ac.uk/sale/bib/main.html#XGri97a">Gri97a</a>): the process of automatically
analysing text (and, less often, speech) to populate fixed-format, unambiguous
data structures such as spreadsheets, link-analysis engines, databases or,
more recently, semantic repositories.</p>


<a name="section-4.1."/><h2>4.1. Annotation: the Missing Manual</h2>

<p>The first thing that we lack is not about technology but about how and where
to apply it.  Despite the breadth and depth of literature describing
algorithms, data structures, evaluation protocols and performance statistics
for annotation technology lacks a clear statement of how to go about
specifying and implementing annotation functionality for a new task or domain.
This methodology must cover at least:</p>
<ul>
<li>how to decide if annotation is applicable to your problem</li>
<li>how to define the problem with reference to a corpus of examples</li>
<li>how to identify similarities with other problems and thence to estimate
  likely performance levels</li>
<li>how to design a balance of adaptation interventions for system setup and
  maintenance</li>
<li>how to measure success</li>
</ul>

<p>(I.e.: we need less work that proposes "develop this great new bit of software
and it will transform our lives", and more that defines "how to implement
robust and maintainable services"!)</p>


<a name="section-4.2."/><h2>4.2. A Division of Labour (Multi-role Methodology)</h2>

<p>Systems like GATE provide tools for computational linguists and language
engineers, but there are other categories of staff that need to be involved in
an annotation factory.</p>



<p>First, annotation of training data for learning algorithms should be a task
requiring little skill beyond that of a computer-literate person. That's a
requirement because training data volumes are typically large and therefore
the labour involved has to be cheap to make the process economic. For the
same reason the annotation environment should be as productive as possible,
for example by bootstrapping the annotation process with mixed-initiative
learning and by providing a voting mechanism for multiple simultaneous
annotators (this is necessary to guarantee quality with low-skilled staff).</p>

<p>Second, data curation or systems administration staff may become involved in
customising extraction systems. These types of people would take a training
course of a week or two, and be expected to use a richer toolset, perhaps
including things like ANNIC-based JAPE rule authoring. They would be able to
maintain the IE system, convene and manage annotation staff and liaise with
language engineers as necessary.</p>



<a name="section-4.3."/><h2>4.3. Unifying Adaptation Interventions</h2>

<p>An IE system is a dynamic object which represents a compromise between
information need and development cost. Almost never do the information need
and the data remain static.  Therefore applications software using IE has to
provide a mechanism to tailor the extraction components to new information
needs and to new types of data.  Up to now, no single mechanism has been
discovered that covers all cases, meaning that IE software has to support a
wide spectrum of adaptation interventions, from skilled computational
linguists, through content administrators and technical authors, to unskilled
end-users providing behavioural exemplars. In each area significant research
streams exist and have made good progress over the past decade or so. What has
not been done is to construct a unified environment in which all the different
adaptation interventions work together in a complementary manner.</p>

<p>The three principal mechanisms for IE adaptation in leading HLT research
streams are:</p>
<ul>
<li>assisted authoring of finite state transduction rules, including: generation
  of rules from annotation pattern searches across corpora (annotations in
  context); graphical debugging of transduction rule execution</li>
<li>supervised mixed-initiative learning of extraction models</li>
<li>unsupervised clustering of terms for purposes such as populating
  domain-specfic gazetteers</li>
</ul>

<p>In addition, underlying the adaptation process are automated measurement and
visualisation tools, and frequency-based document search (or information
retrieval) tools. </p>




<a name="section-4.4."/><h2>4.4. Assistive vs. Autonomous</h2>

<p>A modern factory engaged in the production of high-tech goods combines a large
degree of automation with skilled labour of various types and quantities.
Robots often play a significant role but they are never altogether
unaccompanied: at the very least service engineers must attend their
operation, and in most cases there will also be staff who take care of
reconfiguring robotic equipment for new product lines or refinements to the
existing processes and products.  An annotation factory should take the same
approach: automatic where possible, assistive where necessary.</p>




<a name="section-4.5."/><h2>4.5. Iterative Processes vs. Waterfall or "Big Bang"</h2>

<p>How to construct a failing software project:</p>

<p>The most effective method is to work for the government. If that isn't
possible, another good way is to define your plan as:</p>
<ul>
<li>first we (highly-paid developers) decide what to build (requirements)</li>
<li>then we (middle-level developers) decide how to build it (design)</li>
<li>then we (lower-level developers) implement it (coding)</li>
<li>then we (support staff) deploy it (disaster)</li>
</ul>

<p>A better model: iteration, XP, do something useful now and get it out there</p>

<p>The same with annotation: we need iterative methods that account for
changing needs and gradual refinement of requirements.</p>


<a name="section-4.6."/><h2>4.6. Service-Orientated vs. Monolithic</h2>

<ul>
<li>Software has evolved: modules, objects, components, now services</li>
<li>Gartner reports that</li>
  <ul>
  <li><em>"By 2008, SOA will be a prevailing software engineering practice,
    ending the 40-year domination of monolithic software architecture"</em> </li>
  <li><em>"Through 2008, SOA and web services will be implemented together in
    more than 75 percent of new SOA or web services projects."</em></li>
  </ul>
<li>EAI less costly / more reliable via explicit modelling of meaning of
  data and processes</li>
<li>B2B and B2C eCommerce systems will become:</li>
  <ul>
  <li>more reusable: interfaces meaningful for machines, not just people;</li>
  <li>more dynamic and adaptive, virtual apps composed from discovered
    components;</li>
  <li>more scaleable, distributed across multiple machines;</li>
  <li>saleable as discreet elements in application service providers market</li>
  </ul>
</ul>


<a name="section-4.7."/><h2>4.7. Sustainability, Open Source, Community Support</h2>

<p>Section: why OSS is good; why OSS is not enough; GATE's engineering and
community practices</p>


<a name="section-4.8."/><h2>4.8. General vs. specific</h2>

<p>If we look towards digital scholarship in 10 or 20 years time we should expect
the specificity of the computational tools in use to be much lower than is
currently the case. The history of computer science is of the progressive
capture of patterns that describe families of problems and the implementation
of progressively higher-level languages and software abstractions to
facilitate the creation of problem solutions.<sup><a href="#footnote1">1</a></sup> This process of
generalisation is important for a number of reasons, including the prohibitive
cost of developing each computational technique afresh for each new problem
domain. In an arts and humanities context the generality of the techniques
imported from computing and eScience is a measure of how well those techniques
will support collaboration: the more that tools and data formats diverge, the
harder for researchers to share insights.</p>

<p>There's an important counter-argument to generalisation which relates to the
necessity of systems that are sensitive to domain-specific practice, and as
Greengrass argues in <a href="http://gate.ac.uk/sale/bib/main.html#XGre06a">Gre06a</a> the prospects of fully general ontologies
for subjects like history are remote. We have to show how higher-level
abstractions can serve specific cases, and this means user trials, qualitative
and quantitative evaluation.</p>

<p>The key foundation stones of this edifice are:</p>
<ul>
<li>interoperation</li>
<li>integration</li>
<li>reuse</li>
<li>longevity</li>
</ul>
<p>...</p>


<a name="section-4.9."/><h2>4.9. Grounded</h2>

<p>As Greengrass points out in <a href="http://gate.ac.uk/sale/bib/main.html#XGre06a">Gre06a</a> there cannot be an ontology of
history because so much of that discpline involves sifting and weighing
competing viewpoints in changing contexts and with evolving evidence. In fact
the logical mechanisms deployed in current ontology modelling languages are a
bad fit for the decidedly un-logical (if not illogical) nature of human
discourse when viewed from any sufficiently general perspective, and this
point can be regarded as proven by the results (and frequently lack of them)
from the past half-century of AI research. Therefore we should also not expect
any single ontology of the present or the future to emerge. What then of
semantic annotation? The simple answer is that while there will be no semantic
web singular there is already a percolation of semantics into the web and
other information sharing systems that suggests that multiple partial and
domain specific ontologies are likely to succeed, at least where their mission
is not to replace human faculties but to assist and augment them.</p>

<p>.... e.g. CIDOC CRM, modularity of Proton, ....</p>

<p>Neither are the new semantic web technologies a solution to the well-known
problems of AI....</p>


<a name="section-5."/><h1>5. Conclusion</h1>

<p>To summarise, we <b>have</b> lots and lots of tools and algorithms for annotation;
what we <b>need</b> is</p>

<ol>
<li>methodological instead of purely technological </li>
<li>collaborative and multi-role instead of single role</li>
<li>inclusive of previously disjoint adaptation interventions</li>
<li>assistive instead of autonomous</li>
<li>diachronic, life-cycle orientated, not waterfall or "big bang" orientated</li>
<li>service- and grid- orientated, not monolithic</li>
<li>open source and sustainable</li>
<li>capable of generalisation to many applications</li>
<li>grounded in specific domain practice</li>
</ol>

<p>Another way of thinkng about this work is about how GATE generalises over
R&amp;D-level patterns and how safe generalises over application-level patterns
(see tutorial gleam talk)</p>


<a name="section-6."/><h1>6. Postscript</h1>

<p>Truth is stranger than fiction:</p>

<p><img src="http://gate.ac.uk/sale/images/square-fish.jpg"/></p>

<p>(A real square fish.)</p>


<a name="section-7."/><h1>7. Misc text that may not fit anywhere</h1>

<a name="section-7.1."/><h2>7.1. Semantic Annotation and Human Language Processing</h2>

<p>Automation of Semantic Annotation relies on computational processing of
natural languages, commonly configured in this context as (ontology-based)
Information Extraction (IE).</p>

<p>A widely used system for Information Extraction (and other language processing
tasks) is GATE, a General Architecture for Text Engineering<sup><a href="#footnote2">2</a></sup></p>

<ul>
<li>4 types of IE, OBIE, coreference (de-duplication)</li>
<li>empirical annotation loop: annic, jape, annot diff, obie, ...</li>
<li>Empirical methods and distributed collaborative semantic annotation for
  digital scholarship and research knowledge management.</li>
</ul>


<a name="section-7.2."/><h2>7.2. Implementation</h2>

<p>We have defined semantic annotation quite broadly as the addition of
machine-interpretable data to unstructured information such as text, pictures
or audiovisual streams. This definition allows many computational
instantiations; the configuration that we will discuss here is structured
around an ontology specified in a formal language such as OWL<sup><a href="#footnote3">3</a></sup> and stores instances of concepts, their properties and
relations between them in a semantic repository. The connection between
conceptual data and the unstructured sources is maintained by an annotation
graph which references text via character offsets, pictures via spatial
coordinates and audiovisual material via time codes.</p>


<a name="section-7.3."/><h2>7.3. Semantic Web Standards and Semantic Annotation</h2>

<p>The great strength of XML has been to become a universal language for
structured data exchange. XML specialisations like RDF and OWL
(<a href="http://gate.ac.uk/sale/bib/main.html#XRDF">RDF</a>, <a href="http://gate.ac.uk/sale/bib/main.html#XRDFS">RDFS</a>, <a href="http://gate.ac.uk/sale/bib/main.html#XOWL">OWL</a>) add standardised mechanisms for defining the meaning of
XML data and thus minimise the interpretation overheads involved in data
exchange.</p>

<p><em><b>Un</b></em>structured data resources such as text, pictures or audiovisual streams
are harder to deal with. Semantic Annotation (SA) is about the addition of
machine-interpretable metadata to unstructured resources. This is done in such
a way that the results can be used by computers to enhance the usefulness of
the resources and allow the application of methods previously only possible
for structured data.</p>

<p>The uses of these new semantic technologies include data transformation for
purposes of interoperability and interchange, richer navigation, browsing and
search, and to facilitate open distributed systems composed of knowledge-rich
services. (They do not constitute a general solution to the problems of AI
...)</p>

<p>This will result in technology and demonstration systems with the following
characteristics:</p>
<ul>
<li><b>Collaborative.</b> To use annotation as a facilitator of scholarship,
  discussion and debate; therefore its creation, visualisation and editing
  must be a collaborative and distributed process.</li>
<li><b>Grid enabled.</b> Data grids and computation grids reduce the need for
  organisations to maintain their own computer hardware facilities and help
  turn informatics into a commodity service.</li>
<li><b>VRE enabled.</b> Virtual Research Environments (VREs) exploit ubiquitous
  networks to move discrete and paper-based practices into connected
  electronic environments supporting research <em>in silico</em> that shares
  resources and access points across the internet.</li>
<li><b>Service-based.</b> Gartner reports that "By 2008, SOA<sup><a href="#footnote4">4</a></sup> will be a prevailing software engineering
  practice, ending the 40-year domination of monolithic software
  architecture". Service-based sstems are: more reusable, with interfaces
  meaningful for machines as well as people; more dynamic and adaptive, with
  virtual applications composed from discovered components; more scaleable,
  with processing distributed across multiple machines.</li>
<li><b>Multi-format.</b> Annotation is a good way to deal with format
  incompatibilites from legacy systems and that arise from current closed
  systems. For example we deal with Word and PDF via standoff markup which
  doesn't change the originals but references them via pointers.</li>
<li><b>Supporting annotation creation, analysis and evolution.</b> sophisticated
  query and concordancing over large corpora; pattern matching and machine
  learning for automatic annotation; differencing and measurement; efficient
  distributed storage.</li>
<li><b>Interoperable.</b> Standardised description of both the syntactic or data
  structure level and the semantic or interpretative level of information
  coupled with open software platforms and protocols means minimum overhead
  for interoperation with other systems.</li>
<li><b>Referential.</b> The arts and humanities use reference much more intensively
  and precisely than science and engineering. Quotation, footnoting, citation
  and all the complex apparatus of a critical editing and publication can be
  represented at a structural level as annotation.</li>
</ul>

<h1>Footnotes</h1>
<p><ol>
<li>
<a name="footnote1"/>
There's a parallel
history of corporations over-selling incremental change as revolutionary
advance. Grids and Semantic Webs are certainly an advance but as usual they
are neither panacea nor magic bullet and we must ground our deployment of new
technology with evaluations of its efficacy in use.
</li>
<li>
<a name="footnote2"/>
Anecdotal
support for the popularity of GATE is furnished by Google: of around 250
million web pages containing the word "gate" Google considers
http://gate.ac.uk/ - the home page of GATE - to be the most likely search
result for that word.
</li>
<li>
<a name="footnote3"/>
OWL:
Ontology Web Language.
</li>
<li>
<a name="footnote4"/>
SOA:
  Service-Oriented Archiveture.
</li>
</ol></p>
</body></html>