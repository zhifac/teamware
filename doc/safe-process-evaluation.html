<!-- AUTOGENERATED FILE: ALL EDITS WILL BE LOST!!! -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta charset="UTF-8">
<title>Evaluate the SAFE Process Experimentally</title>
<style type="text/css">
<!--
html, body {
  background: #fff;
  color: #000;
  font-family: sans-serif;
}
h1,h2,h3,h4,h5,p,ul,ol { font-family: sans-serif; }
pre { font-family: monospace; }
h3.navhead {
  font-size: 100%;
}
div.banner {
  border: none;
  margin-right: 0px;
  margin-left: 0px;
  padding: 0.09em;
  text-align: center;
  font-weight: bold; 
}
div.banner a:link, div.banner {
  background: #A0D0F0;
  color: #000000;
}
div.banner a:active {
  background: #000000;
  color: #FFFFFF;
}
div.banner a:hover {
  background: #000000;
  color: #FFFFFF;
-->
</style>
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>Evaluate the SAFE Process Experimentally</h1>


<p>
The SAFE process typically incorporates manually annotating with automatic 
annotation extracting. The SAFE also provides several facilities (e.g. the 
annotation schema dialog, the ANNIC, etc.). We need to figure out how efficient 
of manually annotating, how useful of the aiding facilities, how effective of 
automatic annotation extraction, and the best way of combining the two. 
We assess those aspects of the SAFE process experimentally. Note that  
(
<a href=http://gate.ac.uk/sale/bib/main.html#XDay97>Day97</a>) presented some experimental results for evaluating the 
annotating process using the Alembic system.

<hr>
<h2>1. Accuracy of Automatic Annotation Extracting</h2>


<p>
The SAFE process attempts to do as much work fully automatically as possible.
In the workflow an automatic extraction pipeline pre-processes each
document before sending it to human annotators.  As a result, a document presented
to an annotator has some pre-annotation already for those annotation types
required to be annotated. Then the annotator must check and correct the
pre-annotations, as well as reading the text to identify what additional
annotations are needed. 

<p>
The pre-processing includes two techniques - the JAPE grammars created
manually and a machine learning (ML) model learned automatically from
previously annotated documents and applied to new documents to generate
annotations. In the following we present some experimental results for the
accuracies of the two pre-processing techniques - JAPE grammars and ML.

<p>
<b>TODO</b> Accuracy for JAPE grammar: calculate the F-measures for the results from
the JAPE grammars against the human annotated, for different annotation types.

<p>
<b>TODO</b> Accuracy for ML: the learning curves of ML for different annotation types.

<p>
<b>TODO</b> Compare the costs of JAPE grammar and ML at the same accuracy level:
the cost of ML can be measured by the documents required for training, and the
time spent on annotating those documents. The cost of JAPE grammar can be
measure by the time of creating it. Note however that manual annotation is a
less skilled process than JAPE pattern authoring.

<p>


<hr>
<h2>2. Effectiveness of the SAFE Facilities</h2>


<p>
We can assess the effectiveness of the facilities provided in the SAFE system
and the automatically pre-annotating for helping annotator by comparing the 
time spent by annotator with and without those facilities and/or automatic 
annotating.

<p>
<b>TODO</b> Evaluate the effectiveness of the SAFE annotating tools: compare
the human effort using the SAFE with using other annotating tools (e.g. 
Wordfreak, Jena, Alembic) by annotating the similar documents.

<p>
<b>TODO</b> Evaluate the helpfulness of the pre-processing: compare the human 
effort with and without the pre-processing (e.g. the JAPE grammars and ML).

<p>
The human effort is estimated by the time spent by annotator in
the experiment. The estimates are subject to several sources of inaccuracy. 
First, the annotators were skilled staff who are knowledgeable about GATE 
and SAFE. Second, the length of the documents varies widely (therefore we 
also report a figure averaged over document length in KB). Third, the 
annotation density varies widely (therefore we also report a figure averaged 
over number of annotations).

<p>
In detail we need the following composite figures for each experimental setting:
<ul>
<li>
XXXs per document
</li>
<li>
XXXs per KB 
</li>
<li>
XXXs per annotation
</li>
</ul>

<p>
Source data from which the above figures are obtained:

<p>


<table cellspacing="0" border="1">
<tr><td>
</td><td> Document name </td><td> Starting time </td><td> Finishing time </td><td> Annots before </td><td> Annots after  </td><td> Doc KB </td><td>
</td>
</tr>
<tr>
<td>
</td><td>               </td><td>               </td><td>                </td><td>               </td><td>               </td><td>        </td><td>
</td></tr>
</table>


<p>
When an annotator annotates one document, s/he needs write down the first three 
items in the following table. Other three items can be deduced later from the 
document. The annotating time spent on the document can be computed from the
starting time and finishing time. Note that the starting time is also needed for
assessing the ML in the mixed-initiative annotating, as the ML suggestions
at late stage could be more accurate and hence more useful than those at 
early stage.

<p>
</body>
</html>
