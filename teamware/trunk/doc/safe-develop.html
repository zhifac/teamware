<?xml version="1.0" encoding="utf-8"?>
<!-- AUTOGENERATED FILE: ALL EDITS WILL BE LOST!!! -->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>SAFE Developer Notes</title>
<style type="text/css">
/*<![CDATA[*/
<!--
html, body {
  background: #fff;
  color: #000;
  font-family: sans-serif;
}
h1,h2,h3,h4,h5,p,ul,ol { font-family: sans-serif; }
pre { font-family: monospace; }
h3.navhead {
  font-size: 100%;
}
div.banner {
  border: none;
  margin-right: 0px;
  margin-left: 0px;
  padding: 0.09em;
  text-align: center;
  font-weight: bold; 
}
div.banner a:link, div.banner {
  background: #A0D0F0;
  color: #000000;
}
div.banner a:active {
  background: #000000;
  color: #FFFFFF;
}
div.banner a:hover {
  background: #000000;
  color: #FFFFFF;
-->
/*]]>*/
</style>
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1>SAFE Developer Notes</h1>

<p>Note: this is out of date and needs revising!</p>


<p><h2>Contents</h2>
<p><ul>
<li><a href="#section-1.">1. Introduction</a></li>
<li><a href="#section-2.">2. Overview</a></li>
<li><a href="#section-3.">3. Batch, Mixed Initiative and Active Learning in SAFE</a></li>
<li><a href="#section-4.">4. Description of the modules</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.1.">4.1. Executive module</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.2.">4.2. AnnotatorPool</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.3.">4.3. GATEService (aka GaS)</a></li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#section-4.3.1.">4.3.1. GATE mode</a></li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#section-4.3.2.">4.3.2. Remote DataStore</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.4.">4.4. DocService</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.5.">4.5. Ontology Service</a></li>
<li>&nbsp;&nbsp;<a href="#section-4.6.">4.6. User interfaces</a></li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#section-4.6.1.">4.6.1. Language Engineer UI</a></li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#section-4.6.2.">4.6.2. Annotator UI</a></li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#section-4.6.3.">4.6.3. Curator UI</a></li>
<li><a href="#section-5.">5. Web service APIS</a></li>
<li>&nbsp;&nbsp;<a href="#section-5.1.">5.1. DocService API</a></li>
<li>&nbsp;&nbsp;<a href="#section-5.2.">5.2. Ontology / KB Service</a></li>
<li>&nbsp;&nbsp;<a href="#section-5.3.">5.3. GATEService</a></li>
<li>&nbsp;&nbsp;<a href="#section-5.4.">5.4. AnnotatorPool</a></li>
<li>&nbsp;&nbsp;<a href="#section-5.5.">5.5. Executive</a></li>
<li><a href="#section-6.">6. Build</a></li>
<li>&nbsp;&nbsp;<a href="#section-6.1.">6.1. Building SAFE</a></li>
<li>&nbsp;&nbsp;<a href="#section-6.2.">6.2. Eclipse settings</a></li>
<li>&nbsp;&nbsp;<a href="#section-6.3.">6.3. Executive application installation and build</a></li>
<li>&nbsp;&nbsp;<a href="#section-6.4.">6.4. JBPM Graphical Designer Installation</a></li>
<li>&nbsp;&nbsp;<a href="#section-6.5.">6.5. JBPM build and deploy</a></li>
<li><a href="#section-7.">7. Best practices for SAFE development</a></li>
<li><a href="#section-8.">8. Annex A: workflow for a simple annotation application</a></li>
<li><a href="#section-9.">9. Annex B: workflow for a Machine Learning application</a></li>
</ul></p></p>


<a name="section-1."/><h1>1. Introduction</h1>

<p>This documents contains developer notes for SAFE and describes the different
components of the SAFE architecture.  For an overview of SAFE and its
applications see <a href="http://localhost/create?./gleam-reqs.html">./gleam-reqs.html</a>.  SAFE will be based on a Service
Oriented Architecture (SOA) with different modules communicating via Web
Services.</p>


<a name="section-2."/><h1>2. Overview</h1>

<p>The SAFE architecture will be made of several modules communicating with each
other, as showed on the following figure:</p>

<p><img src="images/gleam-modules2.png"/></p>

<p>The schema above splits the architecture of SAFE into 3 tiers:</p>

<ul>
<li>application side</li>
<li>SAFE core</li>
<li>user GUI's</li>
</ul>

<p>In the user GUIs part, the green boxes correspond to user interfaces
used by different profiles of actors (see <a href="http://localhost/create?./gleam-reqs.html">./gleam-reqs.html</a>). </p>

<p>The core SAFE is made of several modules which communicate via web services, 
the description of which will be given in annex of this document. </p>

<p>Finally, the application realm uses the data generated by SAFE and located in a
DocService or/and in an Ontology Service. These data could be converted and used
directly by an instance of a KIM Server, in which case one could for instance 
reuse the functionalities of the KIM web clients.</p>


<a name="section-3."/><h1>3. Batch, Mixed Initiative and Active Learning in SAFE</h1>

<p>

<p>Note: we should also look at including material from OLLIE:
<a href="http://gate.ac.uk/sale/bib/main.html#XTab03a">Tab03a</a>, <a href="http://gate.ac.uk/sale/hlt03/ollie-sealts.pdf">http://gate.ac.uk/sale/hlt03/ollie-sealts.pdf</a>.</p>

<p>The basic concept of MI approaches is to start with a model which is learnt
from some initial annotations and keep updating the model with new user
annotations. The procedure continues until it reaches a specific accuracy
threshold. Then it is the trained model which generates new annotations for
unseen documents and shows them as suggestions to the annotators. Annotators
then accept or discard, add new or correct the existing ones.  The model gets
updated with the provided feedback and thus generates annotations with
increasing accuracy with each subsequent attempt.</p>

<p>But the process itself is very complicated and there are several issues to be
addressed. The following list highlights some of such issues:</p>

<ul>
<li>What specific type of task the model should be trained for? Should we only
  support some specific tasks or users should be allowed to create new tasks?</li>
<li>What features are best for a given specific task? (Should we allow users to
  choose them? How?)</li>
<li>Is it possible to learn such features automatically?</li>
<li>How to produce such features? What linguistic resources (PRs) are needed and
  which ones to use? Should we allow users to choose them?</li>
<li>What learning algorithm to use for a given task?</li>
<li>How difficult it is to configure a learning algorithm?</li>
<li>Is model updatable?</li>
<li>What would be the optimum threshold before a model starts suggesting new
  annotations?</li>
<li>How to decide which annotations to accept from users and which ones to
  discard?</li>
<li>Do we need post editing rules to correct the results? If yes, what type of
  rules? Which post editing rules to accept and which ones to discard?</li>
<li>What tools are required for allowing users to create and submit new
  annotations?</li>
</ul>

<p>The following sections summarise some of the systems which are relevant to the
discussion. Some of the points raised in the above list are discussed with new
thoughts of improving such systems.</p>

<p>Alembic: The main goal of the Alembic workbench is to provide a natural
language engineering environment for the development of tagged corpora. When
users, after annotating documents, submit their annotations to the system, the
underlying rule based system (Alembic text processing system), tries to induce
new rules from the context of these annotations.  The learned rules are then
presented in a human readable format. During the annotation session, users are
allowed to alter or remove these rules. They are also allowed to introduce new
manual rules.  The analysis of such rules and to monitor their effects on the
corpus, the workbench provides a set of corpus analysis tools (not listed any
at the moment). The outcome of annotating documents within the alembic
workbench is a domain-specific training set that has been acquired with
minimum human effort, a set of information extraction/annotation rules for
annotating unseen data and a better understanding of the data through the use
of corpus analysis techniques.</p>

<p>Callisto: Callisto is a new annotation tool from MITRE which is being
developed as part of the Alembic Workbench.  It provides an environment for
annotating documents aimed at different tasks (e.g. co-reference annotations,
part of speech tagging etc).  For each new session, the system asks users to
choose one of the tasks for which s/he wants to annotate the documents.  Based
on the user’s choice, the relevant environment settings are loaded. For
example to annotate tokens in a document with their POS information user is
provided with a list of POS tags and s/he’s asked to choose one of the values
(something similar to our annotation schemas).</p>

<p>The paper titled “Active Learning for Part-of-Speech Tagging: Accelerating
Corpus Annotation” gives a very good explanation of the concept of Active
Learning.  They explain how they learn a POS tagger using MEMM and how does
the active learning help them to accelerates the efficiency of a human
annotator even with a small amount of training and data.  They explain what
data should be considered for training a model and for what data the users
should be asked for their feedback on. In order to determine what sentences
provide more information and should be used for training, they experiment with
various policies - Query by Committee (QBC), Query by Uncertainty (QBU) (may
be some one needs to explain me this in details L), and their variants - QBUV
(viterbi), QBCV, Weighted QBC and Weighted QBV. They show that QBUV is a cheap
approach to active learning, but QBU performs best when a small amount of data
is available (more details in their paper). They explain that active learning
is an approach to machine learning in which a model is trained with the
selective help of an oracle. The oracle provides labels on “tough” cases, as
identified by the model. Easy cases are assumed to be understood by the model
and assumed to require no additional annotation by the oracle.</p>

<p>Amaya: Amaya is an annotation tool created under the Annotea project.  The
main difference is in the way they store annotations which allows users work
offline. They store annotations separate from the actual document content. It
can be saved either on a server or on a local file system. They use a special
RDF annotation schema to store these annotations. For each annotation they
store, at least, its location (e.g. local file system or annotation server),
scope (e.g. if it is applicable to the entire document or a piece of text),
annotation type (e.g. if it is an annotation, comment or query) and the
author.  Initially annotations are stored on a local system, which can then be
copied over to the server if user instructs to do so.  When a document is
loaded, annotations from the local file are loaded automatically and user is
given an option to show annotations or get relevant annotations from the
server. Amaya allows changing annotation boundaries within the document. It is
a three clicks drama where user first places a cursor where s/he wants to move
annotations and saves it as a stored location and then selects the annotations
and clicks to move to the stored location. This moves all the selected
annotations to the stored position.</p>

<p>S-CREAM: S-CREAM stands for Semi-automatic CREATion of Metadata. From the
first read it looks very similar to the ALEMBIC workbench. The most similar
things are their architecture and the approach for inducing new rules. S-CREAM
is an annotation framework that integrates a learnable information extraction
component (Amilcare).  Amilcare is a tool for adaptive IE from text designed
for supporting active annotation of documents for KM.  The workflow of the
S-CREAM processing is as below:  User annotates the documents manually with
the given ontology.  The information about new entities found in the document
(using the learnt rules from previous training) is extracted. Finally,
anaphora resolution among the found entities and the establishment of new
logical relationships among the entities is derived as part of the discourse
resolution phase.</p>

<p>Rule induction: Its learners induce rules that are able to reproduce the text
annotation. They use Ont-o-Mat to allow users to create manual annotations.
The learner starts inducing wrapper like rules that make no use of linguistic
information where rules are sets of conjunctive conditions on adjacent words.
Then the linguistic information provided by ANNIE is used in order to
generalise rules: conditions on words are substituted with conditions on the
linguistic information (e.g. conditions matching either the lexical category,
or the class provided by the gazetteer etc.). All the generalizations are
tested in parallel by using a variant of the AQ algorithm and the best k
generalizations are kept for IE. The learner induces two types of rules
(tagging rules and correction rules). Tagging rules add mark-ups to the
document where as correction rules decide the correct positions of these
mark-ups if needed.  Similar to Alembic, they also allow users to make
modifications to these rules and in addition they allow balancing precisions
and recall.</p>

<p>Annotation framework: CREAM allows generation of annotation in three different
modes (Note: from paper the last two looked almost same).  Firstly,
annotations by typing statements - this involves working almost exclusively
with the ontology browser and the fact templates and filling up the
information where required.  Secondly, annotation by marking up - this
involves loading data in document editor, marking parts of the document and
then drag and dropping them on the ontology. I think, what they mean here is
that they also allow adding new instances and classes to the ontology.
Finally, annotation by authoring web pages - this involves the reuse of data
from the ontology and the fact browser in the document editor by drag and
drop. I think, they only allow clicking on the existing instances and classes
in the ontology and not adding new resources. It embeds resulting tags in the
HTML file itself but can be stored separately in a file or database.  They
support three annotations mode: 1) Highlighting mode - all annotations are
highlighted in a single click and user is given an option of accepting or
rejecting the annotation 2) Interactive mode - one by one annotation is
highlighted and on each highlight user is asked to either accept or reject the
annotation before proceeding to the next suggestion and 3) Automatic mode -
this is something similar to batch learning (a bunch of documents is processed
altogether) except that when user edits these documents, a feed back is sent
back.</p>

<p>Setting up a new task: S-CREAM is for training a model or learning rules for a
specific domain (user can choose which domain) and therefore user has to
provide an ontology + a set of web-pages specific to that domain for the
training purpose (and for manual annotation purpose) and a subset of ontology
concepts to be learned.</p>

<p>Knowtator: Knowtator is a Protégé plug-in for annotating documents in order to
prepare a gold standard data. It provides an editor to create annotation
schemas, which can be later used while creating annotations.  They allow
various annotators to annotate the same document simultaneously, and when they
all submit annotations back, the annotations in common are considered as
consolidated annotations and the rest are considered or shown to the user for
further reviewing. They provide the detailed analysis of which users agree/or
don't agree on what features of what annotations via Inter-annotator agreement
metrics. They have implemented several match criteria (to be investigated
later) (i.e. what counts as agreement between multiple annotations). The
report based on this reveals the systematic differences. They allow saving
data (along with annotation) or only the annotations separately in a file with
the choice of annotations to be exported.</p>

<p>In the paper titled “Knowledge Extraction by using Ontology based Annotation
Tool”, they describe an approach where the user is presented with a set of
possible tags which could be used during the mark-up process. They use a
system called Crystal to learn new rules. Crystal works using the bottom-up
approach and finds rules for specific instances and generalises them.  Crystal
takes similar instances and generalises them into a more general rule by
preserving the properties from each of the instance definitions. Using these
rules, the Crystal locates the entities of interest in the document, which
automatically become candidates for the annotations. For example, from
patterns "John visits IBM" and "Bill visits Microsoft", Crystal learns a
pattern "X visits Y" and if "John" and "Bill" were annotated as Visitor and
IBM and Microsoft were annotated as Organization, Crystal creates a pattern
"Visitor visits Organization".   They use Badger to fill up these templates.
Badger considers one sentence at a time and tries to fill up these templates.
Finally they do reasoning over ontology to find answers to the question such
as which person visited Microsoft. In order to do that, they have a sub class
called Visitor of the class called Person. From the template Visitor visits
Organization, they know that Bill is a visitor, but they check if visitor is a
subclass of Person. If found any pattern, they say that Bill is a person who
visited the Microsoft.</p>
 

<p>Thoughts</p>

<p>Post editing rules</p>

<p>Alembic is a rule based system whereas not all learners we would want to use
would be rule based.  Hence we cannot expect our system to present the rules
in a human readable format or allow them to alter them.  However, we can allow
users to create post editing rules manually (JAPE grammars and may be ANNIC as
an interface to author) or using something similar to Crystal. Alternatively,
we can start with asking system to annotate a copy of the available gold
standard data, which we can later use to compare the two sets and obtain the
post editing rules for missing and unwanted annotations.</p>


<p>Suggestion rules</p>

<p>The idea here is to find out the general mistakes that users make. For
example, annotating some text T as A where as it should be annotated as B.
Such a behaviour can be detected by asking users to annotate some of the
example documents from gold standard.  Once the user has annotated the
document, the document can be compared with the gold standard document and the
usual mistakes can be identified and shown to the user with a suggestion (e.g.
system thinks this should be X, are you sure it is Y?).</p>


<p>Learn what features to use</p>

<p>When user annotates something in the corpus, we can index them using ANNIC
(which is done automatically now). On time to time basis, run ANNIC to
retrieve patterns for each individual annotation. From these patterns collect
statistics for frequently occurring patterns and make a rule out of it. Create
a rule and then apply it to the clean subset of the training data and thus
collect features for underlying annotations of the patterns and measure
accuracy by comparing the results with that of the training data. This way
user doesn't have to specify the dataset definition, and the features can be
learnt automatically.</p>
 

<p>Multiple tasks at the same time</p>

<p>GATE Document Editor allows us to plug-in various tabs (no idea if document
annotator allows us to do that?) Thus, we can allow users to switch among
various tasks by simply allowing him to click on respective tabs.  Annotations
created in different tabs, can be used for different tasks and sent to
appropriate models.  Thus, allowing users to achieve multiple tasks at the
same time.</p>
 

<p>What annotations to consider for training and suggestions</p>

<p>When a document opens, an option to run pre-set models to generate annotation
suggestions should be provided (this may result into overhead for a user if
the model is not very accurate and therefore should be an optional thing).
In the context of ML Service in SAFE, users will provide a lot of annotations,
but a choice of appropriate data can not only reduce the learning time but
also increase the accuracy of the model being trained (i.e. appropriate use of
filters).  At the same time, users should not be asked for their feedback on
the annotations with higher probabilities but only for the ones with lower
probability. This does surely help in reducing the annotation time.</p>


<p>Required features for a specific task</p>

<p>The annotations for contextual features should be created only when the
annotations are submitted to the model.  The settings for what pre and post
processing applications to execute can be set initially while setting up the
specific task. This can be done either automatically or manually.  In the case
of automatic settings, a table with mappings among available features and
their respective PRs can be maintained.  User is then given an option of
choosing one or more features from this table.</p>
 

<p>Annotation templates and change in boundaries</p>

<p>Although we have an option for expanding or shrinking annotation boundaries,
it doesn’t allow changing the offsets directly.  If we allow this option, we
can actually provide pre-created annotations (e.g. a person with some specific
feature value pairs) to allow users to choose one of them or may be allow
users to create template annotations which they can reuse later for faster
annotation. GATE does have an LR that allows loading an XML Annotation Schema
configuration file, but it will be nice to have a wizard or an editor that
allows creating new annotation schemas at run-time and use them in the
document editor. The idea can be borrowed from Knowtator.</p>
 

<p>What is available?</p>

<p>While developing the YALE-based learning API, Julien and I had implemented a
small MI demo (i.e. OBIEEngine) for which different java classes were written.
Those interfaces would provide us a good example to begin with.</p>

</p>


<a name="section-4."/><h1>4. Description of the modules</h1>

<a name="section-4.1."/><h2>4.1. Executive module</h2>

<p>The SAFE executive module will be accessible as a Web Service. Its role is to
combine the different SAFE modules into an application and provide a link
between them. An Information Curator will configure an Executive via a specific
interface, probably a web application and / or GATE, in order to build a
specific <b>workflow</b>. 
An Executive module can handle one workflow at a time, which means that two
different applications (e.g. MOWER (Mining Opinion from WEb Resources),
News Analysis) will have to
run as different instances of the Executive. A workflow will be created by an
Information Curator, via a dedicated web interface. A workflow can also be
defined via a XML file (as in YALE for instance).</p>

<p>An Executive has also the role of keeping a queue of tasks. A task is
associated to a document (or a set of documents) and a process (e.g.
GATEService) and have a priority. This priority can be used for instance to
force a document to be manually annotated as soon as possible.</p>

<p>A detailed example of workflow is given at the end of this document.</p>


<a name="section-4.2."/><h2>4.2. AnnotatorPool</h2>

<p>An AnnotatorPool represents a set of Human Annotators (HA) used for a specific
application. This module is used by an Information Curator to manage the Human
Annotators, decide how to distribute the documents among them, or define how
to merge their annotations. </p>

<p>Some functionalities will be available to the Executive from this module, like
managing the HAs:</p>
<ul>
<li>Add Annotator (userid)</li>
<li>Remove Annotator (userid)</li>
</ul>

<ul>
<li>Specify the AnnotationSets or Annotation types that a HA can see </li>
</ul>
<p>(done at the initialisation of the Pool). </p>

<p><b>Note</b>: An AnnotationSet for a specific user will be created on each annotated
document instead of duplicating the documents for each HA. The name of the AS
is the ID of the HA.</p>


<a name="section-4.3."/><h2>4.3. GATEService (aka GaS)</h2>

<p>(See also the <a href="gas-user-guide.html">GaS User Guide</a>.)</p>

<p>A GATEService is defined here as a pipeline of GATE PR's which is available
via a WebService. In the SAFE architecture, it is called from an Executive. A
GATEService can be a front-end to a set of GATE engines running on different
servers in order to gain in scalability. The configuration of the GATEService
will specify this behaviour. It is transparent to the external user of the
GATEService how many machines are actually used to produce an annotation.</p>

<p>A GaS can declare required and optional parameters whose values are provided by
the caller.  The parameter values are mapped onto either feature values on the
document being processed by the embedded GATE PRs, or runtime parameter values
of the various PRs that make up the application.  The mapping is defined in a
<em>service definition</em>, an XML file supplied by the GaS creator:</p>
<pre>
&lt;parameters>
  &lt;param name\="depth">
    &lt;runtimeParameter prName\="MyAnnotator" prParam\="maxDepth" />
  &lt;/param>
  &lt;param name\="annotatorName">
    &lt;documentFeature name\="annotator" />
  &lt;/param>
&lt;/parameters>
</pre>

<p>Parameter values are all strings, but when mapping to PR parameters the usual
GATE <tt>Resource.setParameterValue()</tt> is used, which can convert strings into
other types (e.g. URL, Integer, etc.).</p>

<p>The service definition also specifies what annotation sets the service requires
as input and populates as output:</p>
<pre>
&lt;annotationSets>
  &lt;annotationSet name\="Original markups" in\="true" />
  &lt;annotationSet name\="" out\="true" />
  &lt;annotationSet name\="Key" in\="true" out\="true" />
&lt;/annotationSets>
</pre>

<p>Note that a service can use the same set for both input and output, and can use the default annotation set (the second example above).</p>

<p>The traditional GATE environment will have a plugin to connect to a GATEService
as to a normal GATE Application, in that case a GATEService should be able to
take as input the content of a document and not only a reference to it in a
DocumentService.  The GaS parameters would translate to runtime parameters on
the GATEServicePR.</p>

<p>Therefore we can distinguish 2 types of Gas:</p>


<a name="section-4.3.1."/><h3>4.3.1. GATE mode</h3>

<p>In this mode the GaS behaves roughly like a current GATE application, i-e it
takes a document and a set of parameters as input and returns a modified
document. This mode is required in order to incorporate a GaS in a GATE
application, like any other PR.</p>


<a name="section-4.3.2."/><h3>4.3.2. Remote DataStore</h3>

<p>In this mode a GaS does not expect the document as input but only a <b>task</b>, i-e
the location of a
document on a remote DocService, taskID, location of an executive service and a
set of parameter values. When
the document is annotated (or processing fails), the GaS communicates directly
to the executive to tell it that the task is complete (or failed). This mode is
required in SAFE.</p>

<p>A new functionality should be created in GATE in order to be able to generate a
simple service deployment (with everything running on a single server) with a
couple of clicks. The result of this operation will be a WAR file, ready to be
unpacked in a web application server.  If you need a more advanced deployment,
e.g. on a cluster, then more administrative intervention will be required.</p>


<a name="section-4.4."/><h2>4.4. DocService</h2>

<p>The current storage API is defined in the gate.Datastore interface. It is
implemented by SerialDataStore and DatabaseDataStore. The SerialDataStore uses
the JAVA default object serialization mechanism. The DatabaseDataStore has
currently two implementations for Oracle and Posgresql. The organisation of
the databases is isomorphic with the GATE document architecture. A DataStore
has very limited searching functionalities, a LR can be found by its ID, name
or type. It is not possible for instance to search a LR depending on its
content. The retrieval of information is straightforward, a whole LR can be
obtained using the API (e.g. reference to a corpus, document, ontology) but
not a subpart of it.</p>

<p>In the context of SAFE it would be necessary to access a remote datastore
through Web Services. In order to reduce the amount of information exchanged
on the network, it would be better to retrieve only a part of a LR, e.g. an
AnnotationSet and not a whole document.The idea is to improve the current
storage API by adding a search API (a la ANNIC) and thus have RDBMS-like
functionalities (storing/querying/retrieving). The ANNIC application would
then become a thin application querying and displaying the output of a
searchable datastore.  The indexing and querying of the data would be made on
the remote server. </p>

<p>A Searchable DataStore will be accessible through a web service. The
underlying implementation will be hidden to the user, it can be a database,
file serialization or memory based system used internally for the storage. The
datastore can run for instance inside a web service container such as Axis on
a remote server. Some Java classes will be developed to implement the service.</p>

<p>From the client perspective, we could have a new type of DataStore in GATE
which would connect to the remote DocService. The behaviour would be exactly
the same as with the other datastores. Not all functionalities of the
Searchable DataStore have to be implemented at once. The main difference
between the old DataStore and the DocService is that in the later only
documents will be stored, and not any type of serializable GATE resource.</p>


<p><b>Locking mechanism</b></p>

<p>A locking mechanism will be implemented at the AnnotationSet level. A resource
will be locked unless the document is marked as read-only or the content is
obtained in read-only mode (i-e the module requesting knows that it will not
modify a given AnnotationSet). The lock will remain locked until a modified
version is committed, or the lock is explicitly released.</p>

<p>An AnnotationSet could also be obtained as read-only, in which case the module
using it would not be allowed to modify it. The DocService will generate a
taskID and keep internally an information about  whether or not the client is
allowed to modify the resource. In other words, a client using the DocService
will have to ask for the right of modifying a resource by locking it.</p>

<p>The text of a document will not be modifiable. It means that in order to
modify the text of a document, one would have to create a new document and
delete the old one.  <b>How to retrieve the text of the document?</b> Is it
automatically returned with the AnnotationSet every time or can it be accessed
via a specific mechanism?</p>


<a name="section-4.5."/><h2>4.5. Ontology Service</h2>

<p>SAFE will use an existing solution for storing ontologies and knowledge bases
such as OWLIM. The definition of the web service will be based on the Semantic
Repository and Query APIs developed by Ontotext.</p>

<p>The Ontology Service will be accessible from the GATE environment, and its
reference will be passed to GATEServices for processing a corpus/document.</p>


<a name="section-4.6."/><h2>4.6. User interfaces</h2>

<a name="section-4.6.1."/><h3>4.6.1. Language Engineer UI</h3>

<p>A Language Engineer builds GATEServices that are used by a SAFE executive.
SAFE being an architecture, it is independent from the tools, however for our
implementation, we will probably use GATE as a Language Engineer UI.</p>

<p>Some modifications will have to be made in GATE: </p>
<ul>
<li>Accessing a remote SearchableDatastore</li>
<li>Accessing an ontology/kb stored remotely</li>
<li>Publishing a local GATE application as a GATEService</li>
</ul>


<a name="section-4.6.2."/><h3>4.6.2. Annotator UI</h3>

<p>The Annotator UI is used by Human Annotators to connect to an Executive and
get a list of documents to process with regard to a given ontology. The
interface will be used to display documents and annotate them. It will also be
possible to see the content of a given ontology. </p>

<p>Documents will have different priorities (low/normal/high). Depending on the
priority, a document will have a different colour in the list of documents to
process.</p>

<p><b>Idea</b>: When a new high-priority document is added when none currently exists
an interrupt message will be displayed and the HA will start working on the
new document.</p>

<p>This means that the AnnotatorUI will periodically ask the AnnotatorPool
whether a more important document has to be processed.</p>

<p>From an implementation point of view, the Annotator UI will be implemented as
a Java Web Start application, reusing as much as possible SLUG components. By
using JWS, it will be possible to connect directly to an Executive via Web
Services. </p>

<p>The Annotator UI will also cater for localisation, with system messages
available in different languages.</p>


<a name="section-4.6.3."/><h3>4.6.3. Curator UI</h3>

<p>An Information Curator is in charge of setting up an Executive module. This
profile being completely new and not covered yet by any existing resource, a
new set of resources will have to be implemented. The Curator UI will have to
communicate with an Executive service. This UI will be a Web Application (JSP
or Servlets). Some of functionalities such as corpora management will be
provided by GATE. The other functionalities are:</p>
<ul>
<li>managing the AnnotatorPool (e.g. seeing the current status of job queues in
  the active AnnotatorUIs, etc.)</li>
<li>creating corpora (with a list of URLs for instance) and inform the executive
  that they have to be processed by passing the references of the corpus in
  the DocService</li>
<li>configuring the Executive (i-e combining SAFE modules)</li>
</ul>


<a name="section-5."/><h1>5. Web service APIS</h1>

<a name="section-5.1."/><h2>5.1. DocService API</h2>

<p>Here are  the functionalities of a Web Service interface for a DocService.</p>

<table border="1">
<tr><td> <b>name</b>	</td><td> <b>description</b> </td><td> <b>input</b> </td><td> <b>output</b> </td></tr>
<tr><td> listCorpora </td><td> list of available corpora	</td><td>  none </td><td>  list of corpusid </td></tr>
<tr><td> listDocs </td><td> ids of documents stored in a corpus </td><td>  corpusid </td><td>  list of docid
</td></tr>
<tr><td> corpusInfo </td><td> info about a corpus </td><td>  corpusid </td><td>  corpusinfo </td></tr>
<tr><td> newCorpus </td><td> creation of a corpus </td><td>  corpusinfo </td><td>  corpusid </td></tr>
<tr><td> delCorpus </td><td> deletion of a corpus </td><td>  corpusid </td><td>  none </td></tr>
<tr><td> newDoc </td><td> creation of a document </td><td>  corpusid,xmlcontent </td><td>  docid </td></tr>
<tr><td> docInfo </td><td> info about a document </td><td>  docid </td><td>  docinfo </td></tr>
<tr><td> delDoc </td><td> deletion of a document </td><td>  docid </td><td>  none </td></tr>
<tr><td> search </td><td> ids of documents matching a given query </td><td> <p>corpusid, query, start, 
length</p></td><td>list of matches </td></tr>
<tr><td> getContent </td><td> retrieve a specific part of a document (e.g. annotationset - text)</td><td>
docid,list of AnnotationSetsID,readOnly </td><td> xmlcontent, taskID </td></tr>
<tr><td> modifyContent </td><td> <p>modify a specific part of a document, the lock is
automatically</p> </td><td>
removed </td><td> docid,xmlcontent, taskID </td><td> none </td></tr>
<tr><td> releaseLock </td><td> removes the lock on an AnnotationSet </td><td>
docid, taskID </td><td> none </td></tr>
</table>

<p>Definitions</p>
<ul>
<li>corpusid: string of characters</li>
<li>corpusinfo: number of documents in the corpus, date of creation and last
  modification</li>
<li>docid: string of characters</li>
<li>docinfo: (to be defined) - may contain a list of corpusid, an integer
  representing the length of the document.</li>
<li>query: covers queries on text and annotations (?)</li>
<li>match: info about a match for a given search (do be specified). Will
  contain a docid and possibly the offsetstart and offsetend of the hit in the
  document.</li>
<li>xmlcontent: text or annotations represented as XML</li>
<li>taskID: String used to identify a task</li>
<li>AnnotationSetsID: Identificator for an AnnotationSet</li>
<li>readOnly: boolean indicating whether the resource has to be locked</li>
</ul>

<p>Ideas / Issues: </p>
<ul>
<li>modify more than one AnnotationSet?</li>
<li>Have properties for the corpora / documents to allow indexing / querying?</li>
</ul>

<p>The users should be able to issue JAPE queries over the indexed corpus and
retrieve annotation patterns. Below are some examples of JAPE patterns.
Actual patterns can also be a combination of one or more of the following
pattern clauses:</p>
<ul>
<li>String (e.g. "Julien Nioche")</li>
<li>{AnnotationType} (e.g. {Mention})</li>
<li>{AnnotationType == String} (e.g. {Person == "Aswani"})</li>
<li>{AnnotationType.feature == featureValue} (e.g. {Person.gender=="male"})</li>
<li>{AnnotationType1, AnnotationType2.feature == featureValue} (e.g. {Person,
  Token.orth == "upperInitial"})</li>
<li>{AnnotationType1.feature == featureValue, AnnotationType2.feature ==
  featureValue} (e.g. {Person.gender=="male", Token.orth=="allCaps"})</li>
<li>({pattern})*n - zero or upto n occurances of the specified pattern (e.g.
  {Title} ({Token.orth=="upperInitial"})*2)</li>
<li>({pattern})+n - one or upto n occurances of the specified pattern (e.g.
  {Title} ({Token.orth=="upperInitial"})+2)</li>
</ul>


<a name="section-5.2."/><h2>5.2. Ontology / KB Service</h2>

<p>Reuse the existing Ontology API in GATE.</p>


<a name="section-5.3."/><h2>5.3. GATEService</h2>

<p>A GATEService has the following query operations (returning string arrays): </p>
<ul>
<li>getRequiredParameterNames()</li>
<li>getOptionalParameterNames()</li>
<li>getInputAnnotationSetNames()</li>
<li>getOutputAnnotationSetNames()</li>
</ul>

<p>The main meat of the service is in the following two operations:</p>

<ul>
<li>processDocument (XMLcontent, parameters) <br/>
  returns: zero or more annotation sets as XML</li>
<li>processRemoteDocument (execLocation, taskID, dsLocation, docID, parameters,
  asMappings) <br/>
  returns: none</li>
</ul>

<p>processDocument provides the "GATE mode" described above:</p>

<p><b>XMLcontent</b>: a GATE document in GATE XML format.  This is expected to contain
at least the annotation sets required as input by the service.</p>

<p><b>parameters</b>: an array of name/value pairs giving String values for the
parameters expected by this GaS.  It is an error if the parameter list does not
specify a value for each required parameter name, though the value specified
may be null.</p>

<p>It returns an array of name/value pairs where the name is an annotation set
name and the value is the XML representation of that annotation set (as per
<tt>DocumentStaxUtils.writeAnnotationSet</tt>), one entry per output annotation set
name (possibly zero if, for example, the service is training a classifier and
doesn't need to return any output).  It is expected that the caller would
replace the content of each annotation set on the original document with the
content returned from the service.</p>

<p>processRemoteDocument provides the "SAFE mode":</p>

<p><b>execLocation</b>: a URL (xsd:anyURI) giving the location of the executive which
should be informed when the process completes (successfully or unsuccessfully)</p>

<p><b>taskID </b>: task identifier that identifies this task to an Executive.</p>

<p><b>dsLocation</b>: a URL giving the location of the document service in which the
document to be processed resides.</p>

<p><b>docID</b>: the ID of the document in the doc service.</p>

<p><b>parameters</b>: as above</p>

<p><b>asMappings</b>: an array of name/value pairs mapping the annotation sets
expected by the GaS to annotation sets on the document in the doc service.  A
mapping is required for each input and output annotation set name the service
uses.  It is permitted for the same doc-service annotation set name to map to
more than one GaS asName, but there is no guarantee as to the order in which
output sets are written back to the doc service, so don't map two output sets
onto the same set in the doc service.  The purpose of this mapping is to allow
the same service to operate over different annotation sets, for example the
results of different human annotators.</p>

<p>processRemoteDocument does not return anything directly - the success or
failure of the operation is communicated out-of-band to the executive.</p>


<a name="section-5.4."/><h2>5.4. AnnotatorPool</h2>

<p>The following functionalities will have to be available for AnnotatorGUIs.</p>

<ul>
<li>getListOfDocuments (userID)  returns: list of docInfos</li>
<li>finishedTask (taskID)</li>
</ul>

<p>The Executive also needs to assign a task to the Annotator Pool</p>
<ul>
<li>setTask(taskID,documentRef,parameters)</li>
</ul>

<p><b>How to set the parameters? XML? the reference to an ontology can be passed as
a parameter, same for the document</b></p>

<ul>
<li>need to manage the users at this level. This is done by an InfCurator</li>
<li>addAnnotator(userID,password)</li>
<li>removeAnnotator(userID,password)</li>
</ul>

<ul>
<li>parameters such as number of HAs required for each document or how to
  combine the ouput of two different users can be passed to the AnnotatorPool
  as parameters.</li>
</ul>


<a name="section-5.5."/><h2>5.5. Executive</h2>

<p>Needs to receive a message about the completion or failure of a task</p>
<ul>
<li>finishedTask (taskID)</li>
<li>taskFailed (taskID)</li>
</ul>

<p>Has to be configured i-e receive a workflow, which can be represented as an XML
document (to make things simple at the begining). </p>
<ul>
<li>setWorkflow (XML configuration)</li>
</ul>

<p>It can be queried directly by a client, which sends the content of a document: </p>
<ul>
<li>processDoc (XMLcontent)</li>
</ul>

<p>which internally: 
o creates a corpus
o creates a document
o put the document in the corpus
o processes the document
o loads the content of the document in memory
o deletes the temporary corpus
o returns the content as XML</p>

<p>Similarly the operation:</p>
<ul>
<li>processCorpus (corpusInfo)</li>
</ul>

<p>will give the reference to a corpus of documents located in a DocService and
inform the Executive that it has to be processed. That operator might return a
taskID, which could be used to display the evolution of the process.</p>


<a name="section-6."/><h1>6. Build</h1>
<a name="section-6.1."/><h2>6.1. Building SAFE</h2>

<p>SAFE has a top-level build file (safe/build.xml) that calls all the subsidiary
build file targets in the right order to build the complete SAFE system.  When
you first check out SAFE from subversion you will need to do a complete ant
build by running <tt>ant build-all</tt> in the <tt>safe</tt> directory to create all the JAR
files that tie the components together (e.g. the web service client stubs used
to make calls to the doc service, GATE service, etc.).</p>

<p>Libraries used by SAFE components are stored in directories under <tt>safe/lib</tt>.
Generated JAR files such as <tt>docservice-client</tt> are placed in directories under
<tt>safe/lib/exports</tt> - this directory is (deliberately) not under version
control.</p>
<a name="section-6.2."/><h2>6.2. Eclipse settings</h2>

<p>To use the SAFE projects in Eclipse you will need to set a <em>classpath variable</em>
in Eclipse to enable the project files to find their required libraries.  Open
your Eclipse preferences (Window/Preferences) and go to Java/Build
Path/Classpath Variables.  Create a variable called <tt>VCS_TOP</tt>, and point it to
the directory in which you have checked out <tt>safe</tt> (and <tt>gate</tt>, <tt>gate-extras</tt>
etc.).  The SAFE component project files are set up to find their dependencies
under <tt>VCS_TOP/safe/lib</tt>.</p>

<a name="section-6.3."/><h2>6.3. Executive application installation and build</h2>

<ul>
<li>Create empty schema in your MySQL DB (executive_db)</li>
<li>copy build.properties to executive-build.properties</li>
<li>modify entries in executive-build.properties to match your environment</li>
<li>execute ant setup-db (that will create tables and data in your DB)</li>
<li>execute ant deploy</li>
<li>start tomcat</li>
</ul>

<a name="section-6.4."/><h2>6.4. JBPM Graphical Designer Installation</h2>

<ul>
<li>Download <a href="http://www.eclipse.org/downloads/download.php?file=/eclipse/downloads/drops/R-3.2-200606291905/eclipse-SDK-3.2-win32.zip">eclipse-SDK-3.2-win32.zip</a> place downloaded file in 'jbpm-designer' folder</li>
<li>Go to 'jbpm-designer' folder and execute ant (default target)</li>
<li>Execute designer.bat in 'jbpm-designer' folder. If Eclipse is started up, everything should be OK</li>
<li>You can learn how to design WF processes in JPDL by watching nice <a href="http://docs.jboss.com/jbpm/v3/demos/movies/jbpm-overview.htm">demo</a> </li>
</ul>

<a name="section-6.5."/><h2>6.5. JBPM build and deploy</h2>
<ul>
<li>edit hibernate.properties in 'jbpm-db/mysql' (4 entries to match your DB settings). By default it is MySQL DB called executive_db.</li>
<li>edit 'create.db.hibernate.properties' in jbpm/src/resources (4 entries to match your DB settings). By default it is MySQL DB called executive_db.</li>
<li>Open 'hibernate.cfg.xml' from  'jbpm/src/config.files' and edit 4 entries to match your DB settings. By default it is MySQL DB called executive_db.</li>
<li>copy build.properties into local.properties and edit ant.home, jbpm.home,  (the folder where this file resides)</li>
<li>To set a DB run ANT script 'ant -buildfile build.deploy.xml create.db', from 'jbpm' folder. </li>
<li>In development run ANT script 'ant -buildfile build.deploy.xml deploy.jbpm', from 'jbpm' folder.</li>
</ul>

<a name="section-7."/><h1>7. Best practices for SAFE development</h1>

<ul>
<li>All build files should <tt>&lt;import file="../lib/lib.xml" /></tt> (or whatever the
  appropriate relative path is from the build file).</li>
<li>Construct your classpaths using the patternsets defined in <tt>lib.xml</tt>.</li>
</ul>
<pre>
&lt;path id="classpath">
  &lt;!-- lib.dir is a property defined by lib.xml containing the absolute
       path to safe/lib -->
  &lt;fileset dir="${lib.dir}">
    &lt;patternset refid="pattern.gate" />
    &lt;patternset refid="pattern.docservice-proxy-api" />
  &lt;/fileset>
&lt;/path>
</pre>

<ul>
<li>If your code depends on a library that is exported by another component then
  you should place a <em>require</em> task, e.g. <tt>&lt;require lib="docservice-proxy-api"
  /></tt> at the appropriate place (typically just before the relevant <tt>&lt;javac></tt>).
  This will stop the build with a useful message if the required library has
  not yet been built.</li>
<li>If your code uses <tt>docservice-proxy</tt>, <tt>executive-proxy</tt> or <tt>gas-safe-client</tt>,
  then you should only depend on the relevant <tt>-api</tt> lib at compile time.  The
  <tt>-impl</tt> JAR should only be included at runtime, or when building a WAR, etc.</li>
<li>Since the <tt>&lt;patternset></tt>s defined in lib.xml are relative to <tt>${lib.dir}</tt> you
  will have to copy lib files into a staging directory before building a WAR
  file.  The following will not work:</li>
</ul>
<pre>
&lt;war destfile="...">
  &lt;lib dir="${lib.dir}">
    &lt;patternset refid="pattern.gate" />
  &lt;/lib>
&lt;/war>
</pre>
<p>as it will put the JAR files into <tt>/WEB-INF/lib/gate-4.0/...</tt> instead of
directly in <tt>/WEB-INF/lib</tt>.  Instead you must copy the JARs into a temporary
directory first:</p>
<pre>
&lt;!-- note the flatten="true" in the following copy task -->
&lt;copy todir="${webinf-lib}" flatten="true">
  &lt;fileset dir="${lib.dir}">
    &lt;patternset refid="pattern.gate" />
  &lt;/fileset>
&lt;/copy>
&lt;war destfile="...">
  &lt;lib dir="${webinf-lib}" />
&lt;/war>
</pre>

<a name="section-8."/><h1>8. Annex A: workflow for a simple annotation application</h1>

<p>We consider here the case of the MOWER application  
(see <a href="http://localhost/create?./gleam-app.html">./gleam-app.html</a>) and present the different interactions between the
modules.</p>

<p>We assume that a remote OntologyService exists already,
containing an ontology of products and more specificaly of digital cameras,
along with a knowledge base of already identified entities (e.g. camera models).</p>

<p>We first describe the case where a GATEService is already available for
annotating the documents with regard to an ontology. In this configuration, a
pool of Human Annotators is available to correct the output of the system. The
GATEService used here has also a module which gives a score to a document,
depending on the number of annotations found in the document in relation to
the size of the document. This information will be used e.g. to determine
whether a document contains too little annotations and thus has to be checked
manually by a HA.</p>

<p>The AnnotatorPool is set up to have only one manual Annotation per document.
The workflow described below can be seen as:</p>

<p><img src="images/executive-scenario1.png"/></p>

<p>Here are the different steps for this application: </p>

<ul>
<li>the Information Curator wants to create an initial corpus. There are several
  options: </li>
  <ul>
  <li>he sets up a GATEService to crawl the web. This GATEService will populate
    a corpus on a DocService. The reference of the remote corpus is given as a
    parameter</li>
  <li>the documents are already available localy. The IC can create a corpus on
    a DocService and add the documents via a specific web interface, or GATE,
    provided that there is a docService client implementation for GATE,
    extending the DataStore API.</li>
  </ul>
<li>as a result the documents are now accessible via the DocService</li>
<li>the Executive module contacts the GATEService and gives him the parameters
  it requires for each document of the corpus such as its location on a
  DocService and the reference to an OntologyService. Each time the Executive
  calls the GATEService, it also passes an ID which corresponds to a Task
  which is stored in a stack by the Executive.</li>
<li>A Task is defined by an ID and corresponds roughly to the state of a
  document in the workflow</li>
<li>The GATEService might internally dispatch the Documents among several
  machines in order to speed up the process. However this is transparent for
  the Executive</li>
<li>The GATEService loads each document from the DocService and annotates it,
  using information from the OntologyService</li>
<li>Every time a document has been processed, it is saved in the DocService</li>
<li>For each document, the GATEService informs the Executive that it has
  completed its task by sending the task ID.</li>
<li>For each task ID received, the Executive proceeds to the next step in the
  workflow by creating a new Task for the corresponding document</li>
<li>In this application, the Executive is set up to check the value of a feature
  in the document and send it to the manual annotation if necessary</li>
<li>In order to check the value of the feature, the Executive needs to connect
  to the DocService and ask it to return the value of the feature for the
  document (or returns the whole document  - the executive will search for it)</li>
<li>If the value of the features does not require a manual annotation, no
  further task is created for the document, which is stored in the DocService
  and can be used in a end-user application</li>
<li>If a manual annotation is required, a task is created for the document</li>
<li>The Executive sends the information about the document and the ontology to
  the AnnotatorPool, along with a task ID.</li>
<li>The AnnotatorPool keeps an internal stack of tasks for the HAs</li>
<li>When an AnnotatorGUI connects to the AnnotatorPool and asks for a task, the
  AnnotatorPool gives him a reference to the document in the DocService and
  points to the OntologyService. It also gives a task ID.</li>
<li>The AnnotatorGUI fetches the information it needs from the DocService, i-e
  the text, plus the AnnotationSet where the ontology entities are stored
  (<b>how does it know about it? A parameter?</b>)    </li>
<li>The AnnotatorUI copies the AnnotationSet into the AnnotationSet of the user
  (<b> or is it done by the AnnotatorPool? Avoid locking the original AS</b>)</li>
<li>the document is manually annotated</li>
<li>When the manual annotation is finished, the AnnotatorGUI saves the
  AnnotationSet of the user in the DocService and returns the taskID to the
  Executive</li>
<li>There is only one HA to be used in this application, so there is no need for
  merging the data or making sure that the results are correct</li>
<li>The AnnotatorPool saves the user AS as the original AS where the entities
  where located, the AnnotatorAS is kept</li>
<li>The AnnotatorPool returns the taskID to the Executive to inform that the
  task has been finished</li>
</ul>

<p>This application would be set up with the following XML file: </p>

<pre>
&lt;workflow>
&lt;globalParam name\="ontology"
  value\="http://onto.dcs.shef.ac.uk:8080/OntoService?name\=cameras"/>
&lt;service id\="S1"
  url\="http://pebble.dcs.shef.ac.uk:8080/GATEService?name\=preprocess" />
&lt;!-- Connects to an Annotator Pool -->
&lt;service id\="S2"
    url\="http://pebble.dcs.shef.ac.uk:8080/AnnotatorPool?name\=cameras">
  &lt;condition feature\="needsManualAnnotation" value\="true"/>
  &lt;param name\="minimumAnnotors" value\="1"/>
  &lt;param name\="inputAS" value\="automatic"/>
  &lt;param name\="outputAS" value\="manual"/>
&lt;/service>
&lt;/workflow>
</pre>


<a name="section-9."/><h1>9. Annex B: workflow for a Machine Learning application</h1>

<p><img src="images/executive-scenario2.png"/></p>

<p>In the following application, a GATEService is used to pre-annotate a corpus
before passing it to an AnnotatorPool. This preprocessing may for instance
recognize some simple entities using a set of preexisting resources (e.g.
Gazeteers). A ML process is used as a separate GATEService and is used before
the manual annotation in order to produce annotations if a model has been
generated. The manual annotation is provided by several HAs, with an
inter-annotator agreement measure done by the AnnotatorPool in order to have
an idea of the quality of the output. The GATEService containting the Machine
Learning resource takes the output to build a model. </p>

<p>Note that a ML resource can be combined with other resources inside a
GATEService. However, in the application described here, we need to use the
same resource for classifying the text (before the manual annotation) and
learning (after the manual annotation).</p>

<ul>
<li>A Corpus is stored on a DocService</li>
<li>For each document of the corpus, the Executive sends its location, a
  reference to an ontology, a set of parameters and a taskID to a GATEService
  for the preprocessing</li>
<li>Every time a document is processed the GATEService informs the Executive
  that the task is completed by returning the taskID</li>
<li>The Executive creates a new task and sends all the required information to
  the GATEService in charge of the learning</li>
<li>One of the parameters of the GATEService specifies that the learning PR must
  be used for classifying (and not training)</li>
<li>The GATEService uses the machine learning. If the system is able to generate
  annotations (the confidence threshold is higher than a parameter specified
  by the application), these are added to the document in the DocService</li>
<li>The GATEService then returns the taskID to the Executive</li>
<li>The Executive sends the information about the document and the ontology to
  the AnnotatorPool, along with a task ID.</li>
<li>The AnnotatorPool keeps an internal stack of tasks for the HAs</li>
<li>In this application, the InformationCurator specifies that a document has to
  be annotated by two different HAs</li>
<li>The task about the document to be annotated keeps an information about who
  already worked on a given document, so that a document is not sent twice to
  the same HA</li>
<li>When an AnnotatorGUI connects to the AnnotatorPool and asks for a task, the
  AnnotatorPool gives him a reference to the document in the DocService and
  points to the OntologyService. It also gives a task ID.</li>
<li>The AnnotatorGUI fetches the information it needs from the DocService, i-e
  the text, plus the AnnotationSet where the ontology entities are stored</li>
<li>The AnnotatorUI copies the AnnotationSet into the AnnotationSet of the user</li>
<li>The document is manually annotated</li>
<li>When the manual annotation is finished, the AnnotatorGUI saves the
  AnnotationSet of the user in the DocService and returns the taskID to the
  Executive</li>
<li>The same document is then annotated by a different person, using the same
  procedure</li>
<li>The document is then available on the DocService with the annotation for
  both human annotators stored in different AnnotationSets</li>
<li>When the number of annotation required has been reached, the AnnotatorPool
  computes an AnnotatorAgreement measure and determines which annotations are
  to be kept</li>
<li>Note: The AnnotatorAgreement measure could be achieved by a GATEService and
  not by the AnnotatorPool</li>
<li>These annotations (found by all annotators) are copied to the target
  AnnotationSet</li>
<li>The users AnnotationSets are kept</li>
<li>The AnnotatorPool returns the taskID to the Executive to inform that the
  task has been finished for this document</li>
<li>The Executive creates a new Task for the document and sends all the
  information required to the GATEService (the one doing the ML). One of the
  parameters of the GATEService specifies that the learning PR must learn from
  the annotationSet</li>
<li>The GATEService then informs the Executive that it has finished with the
  document by returning the taskID</li>
</ul>

<p>This would probably be represented by the following XML configuration file: </p>

<pre>
&lt;workflow>
&lt;globalParam name\="ontology"
  value\="http://onto.dcs.shef.ac.uk:8080/OntoService?name\=cameras"/>
&lt;service id\="S1"
  url\="http://pebble.dcs.shef.ac.uk:8080/GATEService?name\=preprocess">
&lt;/service>
&lt;service id\="S2" url\="http://ml.dcs.shef.ac.uk:8080/GATEService?name\=ml">
  &lt;param name\="learn" value\="false"/>
   &lt;param name\="confidencethreshold" value\="0.7"/>
   &lt;param name\="outputAS" value\="automatic"/>
&lt;/service>
&lt;!-- Connects to an Annotator Pool -->
&lt;service id\="S3"
  url\="http://pebble.dcs.shef.ac.uk:8080/AnnotatorPool?name\=cameras">
   &lt;param name\="minimumAnnotors" value\="2"/>
   &lt;param name\="fusion" value\="1"/>
   &lt;param name\="inputAS" value\="automatic"/>
   &lt;param name\="outputAS" value\="manual"/>
&lt;/service>
&lt;service id\="S4" overrides\="S2">
   &lt;param name\="learn" value\="true"/>
&lt;/service>
&lt;/workflow>
</pre>
</body></html>